{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59c9a82a-5b27-41a5-ba16-59d268a7bff4",
   "metadata": {},
   "source": [
    "## X. Two-stage XGBoost (frequency × severity)\n",
    "\n",
    "In my main XGBoost notebook I already approximate claim counts directly with a\n",
    "single Poisson model (`count:poisson`), which implicitly combines:\n",
    "\n",
    "- the probability of having at least one claim, and  \n",
    "- the expected number of claims, conditional on having a claim.\n",
    "\n",
    "In this notebook I implement a **clean two-stage (hurdle) decomposition** of\n",
    "the same idea:\n",
    "\n",
    "1. A **frequency model** (`XGBClassifier`) that estimates  \n",
    "   \\\\( p(\\text{ClaimNb} > 0 \\mid X) \\\\),\n",
    "2. A **severity-on-positives model** (`XGBRegressor` with Poisson objective)\n",
    "   fitted only on policies with \\\\( \\text{ClaimNb} > 0 \\\\),\n",
    "3. The final expected count as  \n",
    "   \\\\[\n",
    "   \\hat{\\mu}(X) = \\hat{p}(\\text{ClaimNb} > 0 \\mid X)\\times\n",
    "                   \\hat{\\mathbb{E}[\\text{ClaimNb} \\mid \\text{ClaimNb} > 0, X]}.\n",
    "   \\\\]\n",
    "\n",
    "This two-stage implementation is **slightly cleaner and more modular** than the\n",
    "earlier hurdle-style experiment in the main notebook (separate tuning for the\n",
    "classifier and for the positive-claim regressor, with optional tail-weighting).\n",
    "\n",
    "However, the **out-of-sample results are very similar** to the single\n",
    "Poisson XGBoost model in terms of Poisson deviance and RMSE.  \n",
    "In other words, the two-stage approach mainly serves as a **validation check**\n",
    "and an alternative modelling view (frequency × severity), rather than a clear\n",
    "performance improvement for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "105485c1-d93b-4648-b158-46dd5aa001af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((542410, 13),\n",
       " (135603, 13),\n",
       " np.float64(0.05023506203794178),\n",
       " np.float64(0.050234876809510116))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    mean_poisson_deviance,\n",
    "    root_mean_squared_error,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    make_scorer,\n",
    ")\n",
    "\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "\n",
    "target_col = \"ClaimNb\"\n",
    "exposure_col = \"Exposure\"\n",
    "id_col = \"IDpol\"\n",
    "\n",
    "feature_cols = [c for c in df.columns if c not in [target_col, exposure_col, id_col]]\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df[target_col]\n",
    "exposure = df[exposure_col]\n",
    "\n",
    "y_has_claim = (y > 0).astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test, exp_train, exp_test, y_train_has, y_test_has = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    exposure,\n",
    "    y_has_claim,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_has_claim,\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape, y_train_has.mean(), y_test_has.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc5f15b4-f2a7-4f26-a416-da6e0b24b7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features: ['Area', 'VehBrand_lumped', 'VehGas', 'Region_lumped', 'DrivAgeBand', 'VehAgeBand', 'BonusMalusBand']\n",
      "Numeric features: ['VehPower', 'VehAge', 'DrivAge', 'BonusMalus', 'Density', 'logDensity']\n"
     ]
    }
   ],
   "source": [
    "cat_features = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "num_features = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "\n",
    "print(\"Categorical features:\", cat_features)\n",
    "print(\"Numeric features:\", num_features)\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", \"passthrough\", num_features),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "poisson_scorer = make_scorer(mean_poisson_deviance, greater_is_better=False)\n",
    "\n",
    "skf = StratifiedKFold(\n",
    "    n_splits=5,\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccb26e2e-a92a-4b0a-9d49-7349fdc85726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "Best frequency (classifier) params: {'model__subsample': 0.8, 'model__n_estimators': 700, 'model__min_child_weight': 3, 'model__max_depth': 5, 'model__learning_rate': 0.03, 'model__colsample_bytree': 0.8}\n",
      "Best CV AUC: 0.6575288431969822\n"
     ]
    }
   ],
   "source": [
    "xgb_clf = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    tree_method=\"hist\",\n",
    "    device=\"cuda\",\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "clf_pipe = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", xgb_clf),\n",
    "])\n",
    "\n",
    "param_clf = {\n",
    "    \"model__n_estimators\":     [300, 500, 700],\n",
    "    \"model__max_depth\":        [3, 4, 5],\n",
    "    \"model__learning_rate\":    [0.03, 0.05, 0.07],\n",
    "    \"model__subsample\":        [0.8, 1.0],\n",
    "    \"model__colsample_bytree\": [0.8, 1.0],\n",
    "    \"model__min_child_weight\": [1, 3, 5],\n",
    "}\n",
    "\n",
    "clf_search = RandomizedSearchCV(\n",
    "    estimator=clf_pipe,\n",
    "    param_distributions=param_clf,\n",
    "    n_iter=25,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=skf,\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "clf_search.fit(\n",
    "    X_train,\n",
    "    y_train_has,\n",
    "    model__sample_weight=exp_train,\n",
    ")\n",
    "\n",
    "print(\"Best frequency (classifier) params:\", clf_search.best_params_)\n",
    "print(\"Best CV AUC:\", clf_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c49421c-1aa7-44af-b823-25a7f47ecc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency model – Test AUC: 0.6588\n",
      "Frequency model – Test AP:  0.1116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ml forecasting\\.venv\\Lib\\site-packages\\xgboost\\core.py:774: UserWarning: [21:02:36] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:62: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  return func(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "best_clf = clf_search.best_estimator_\n",
    "\n",
    "p_test = best_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "auc = roc_auc_score(y_test_has, p_test)\n",
    "ap = average_precision_score(y_test_has, p_test)\n",
    "\n",
    "print(f\"Frequency model – Test AUC: {auc:.4f}\")\n",
    "print(f\"Frequency model – Test AP:  {ap:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e070520-dfd7-4652-8a7e-5db8b00fa12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train policies: 542410\n",
      "Train policies with ClaimNb > 0: 27248\n",
      "Mean ClaimNb among positives: 1.059380504991192\n"
     ]
    }
   ],
   "source": [
    "mask_pos = y_train > 0\n",
    "\n",
    "X_train_pos = X_train[mask_pos]\n",
    "y_train_pos = y_train[mask_pos]\n",
    "exp_train_pos = exp_train[mask_pos]\n",
    "\n",
    "print(\"Train policies:\", len(y_train))\n",
    "print(\"Train policies with ClaimNb > 0:\", len(y_train_pos))\n",
    "print(\"Mean ClaimNb among positives:\", y_train_pos.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d85315ed-87d1-42d2-8fc4-6e3700f21619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Best severity params: {'model__subsample': 0.9, 'model__reg_lambda': 10.0, 'model__reg_alpha': 0.0, 'model__n_estimators': 700, 'model__min_child_weight': 3, 'model__max_depth': 6, 'model__learning_rate': 0.05, 'model__gamma': 0.3, 'model__colsample_bytree': 0.9}\n",
      "Best severity CV Poisson dev: -0.04877772033214569\n"
     ]
    }
   ],
   "source": [
    "xgb_sev = XGBRegressor(\n",
    "    objective=\"count:poisson\",\n",
    "    tree_method=\"hist\",\n",
    "    device=\"cuda\",\n",
    "    eval_metric=\"poisson-nloglik\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "sev_pipe = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", xgb_sev),\n",
    "])\n",
    "\n",
    "param_sev = {\n",
    "    \"model__n_estimators\":      [300, 500, 700],\n",
    "    \"model__max_depth\":         [3, 4, 5, 6],\n",
    "    \"model__learning_rate\":     [0.03, 0.05, 0.07],\n",
    "    \"model__subsample\":         [0.8, 0.9, 1.0],\n",
    "    \"model__colsample_bytree\":  [0.7, 0.9, 1.0],\n",
    "    \"model__min_child_weight\":  [1, 3, 5],\n",
    "    \"model__gamma\":             [0.0, 0.1, 0.3],\n",
    "    \"model__reg_lambda\":        [1.0, 5.0, 10.0],\n",
    "    \"model__reg_alpha\":         [0.0, 0.5, 1.0],\n",
    "}\n",
    "\n",
    "tail_weight = np.where(y_train_pos >= 3, 3.0, 1.0)\n",
    "sev_sample_weight = exp_train_pos * tail_weight\n",
    "\n",
    "sev_search = RandomizedSearchCV(\n",
    "    estimator=sev_pipe,\n",
    "    param_distributions=param_sev,\n",
    "    n_iter=30,\n",
    "    scoring=poisson_scorer,\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=123,\n",
    ")\n",
    "\n",
    "sev_search.fit(\n",
    "    X_train_pos,\n",
    "    y_train_pos,\n",
    "    model__sample_weight=sev_sample_weight,\n",
    ")\n",
    "\n",
    "print(\"Best severity params:\", sev_search.best_params_)\n",
    "print(\"Best severity CV Poisson dev:\", sev_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eeb01e58-5ac9-48d8-b208-5772f04c391c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two-stage XGB Test Poisson deviance: 0.3064\n",
      "Two-stage XGB Test RMSE:             0.2385\n"
     ]
    }
   ],
   "source": [
    "best_sev = sev_search.best_estimator_\n",
    "\n",
    "# 1) P(has_claim | X)\n",
    "p_hat_test = best_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 2) E[ClaimNb | ClaimNb>0, X]\n",
    "sev_hat_test = best_sev.predict(X_test)\n",
    "sev_hat_test = np.clip(sev_hat_test, 1e-6, None)\n",
    "\n",
    "# 3) Final prediction\n",
    "y_hat_two_stage = p_hat_test * sev_hat_test\n",
    "\n",
    "two_stage_poisson_dev = mean_poisson_deviance(y_test, y_hat_two_stage)\n",
    "two_stage_rmse = root_mean_squared_error(y_test, y_hat_two_stage)\n",
    "\n",
    "print(f\"Two-stage XGB Test Poisson deviance: {two_stage_poisson_dev:.4f}\")\n",
    "print(f\"Two-stage XGB Test RMSE:             {two_stage_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e5113f-5a11-47ef-8b41-72ad781fd4c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
